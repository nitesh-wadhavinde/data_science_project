open anaconda prompt
cd project path
code . which opens vs code instance
now open terminal in vscode
creating new environment
conda create -p venv python==3.8 -y
cls  clear screen
conda activate venv/

next we need to clone this repo and sync with github


this ode would be available on GitHub once you create repo:
echo "# data_science_project" >> README.md
git init
git add README.md

now use git config user.email "email"
git config user.name "username"

git commit -m "first commit"


now we need to push this file

git branch -M main
git remote add origin https://github.com/nitesh-wadhavinde/data_science_project.git
git push -u origin main


goto GitHub and create a new file with name .gitignore hen choose python and commit changes

now to get updated stuff on remote use git pull

now in vscode create a new file setup.py and requirements.txt
with help of setup.py we will be able to build our application as a package and even deploy in pypi

in setup.py:
from setuptools import find_packages,setup
from typing import List
def get_requirements(file_path:str)->List[str]:
	'''
	this func will return list of requirements
	'''
	requirements=[]
	with open(file_path) as file_obj:
		requirements=file_obj.readlines()
		requirements=[req.replace("\n","") for req in requirements]
	

setup(
name='mlproject',
version='0.0.1',
author='Nitesh',
author_email='niteshwadhavinde.nw@gmail.com',
packages=find_packages(),
install_requires=get_requirements('requirements.txt')
)



for setup.py how will it be able to able to find packages and all
for this what we do is create a src folder with __init__.py


so when we are trying to install all in requirements.txt it should run setup.py
to that we add 
-e . in requirements.txt 

now to make sure that the lit we made before removes this -e .
we add below condition o our function get_req
if '-e .' in requirements:
 requirements.remove('-e .')



2.Src folder with package
pip install -r requirements.txt
done

now git add.

git commit -m setup
git push -u origin main


###############################################################################################################
Now we have project structure and logging and exception handling


in src create folder components and under that create __init__.py as it can bei mported as package




components are like all the modules we are going to create like initially we need code to do data ingestion which basically to read data from a db
create another folder named pipeline which has two pipelines training.py and prediction.py, init

now inside src we need to create 3 essential py files
1.logger.py
2.exception.py
3.utils.py(functionalities that can be used anywhere in a database)


now lets first write exception.py
for that search exceptions python documentation

import sys
from src.logger import logging

def error_message_detail(error,error_detail:sys):
    _,_,exc_tb=error_detail.exc_info()
    file_name=exc_tb.tb_frame.f_code.co_filename
    error_message="Error occured in python script name [{0}] line number [{1}] error message[{2}]".format(
     file_name,exc_tb.tb_lineno,str(error))

    return error_message

    

class CustomException(Exception):
    def __init__(self,error_message,error_detail:sys):
        super().__init__(error_message)
        self.error_message=error_message_detail(error_message,error_detail=error_detail)
    
    def __str__(self):
        return self.error_message



now comes the logger.py
 
used to log errors
import logging
import os
from datetime import datetime

LOG_FILE=f"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log"

logs_path=os.path.join(os.getcwd(),"logs",LOG_FILE)


#NOW TO KEEP appending logs
os.makedirs(logs_path,exist_ok=True)

LOG_FILE_PATH=os.path.join(logs_path,LOG_FILE)



logging.basicConfig(
    filename=LOG_FILE_PATH,
    format="[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,


)


git add .
git commit -m "logging exception"
###############################################################################33




now we will focus on  EDA
In the context of data science, EDA stands for Exploratory Data Analysis. EDA is an approach to analyzing datasets to summarize their main characteristics, often using visual methods. Here are some key points about EDA:

Understanding Data: EDA helps in understanding the underlying patterns, structures, and anomalies in the data. This includes identifying the distribution of variables, relationships between variables, and detecting outliers.

Initial Insights: It provides initial insights that guide the selection of appropriate modeling techniques and the subsequent steps in the data analysis process.

Data Cleaning: EDA is crucial for data cleaning. It helps in identifying missing values, erroneous data, and other issues that need to be addressed before modeling.

Visualization: Visualization tools are extensively used in EDA. Common visualizations include histograms, box plots, scatter plots, and bar charts, which help in revealing the data's characteristics and relationships.

Statistical Analysis: Basic statistical techniques are often employed during EDA. This can include calculating measures of central tendency (mean, median), dispersion (variance, standard deviation), and correlations.

Hypothesis Generation: EDA can generate hypotheses about the data. For instance, it may suggest potential predictors for a target variable or reveal patterns that require further investigation.

Steps in EDA
Data Collection: Gathering the data from various sources.
Data Cleaning: Handling missing values, removing duplicates, correcting errors.
Data Profiling: Generating summary statistics and visualizations to understand the data distribution and relationships.
Visualization: Using charts and plots to explore data visually.
Hypothesis Testing: Formulating and testing hypotheses based on the observed data patterns.
Common EDA Techniques
Summary Statistics: Mean, median, mode, standard deviation, quartiles.
Visualizations:
Histograms: To understand the distribution of a single variable.
Box Plots: To visualize the distribution and identify outliers.
Scatter Plots: To explore relationships between two variables.
Correlation Matrices: To examine relationships among multiple variables.
Bar Charts: To compare categorical data.
Data Transformation: Normalization, standardization, log transformations to handle skewed data.
Tools and Libraries for EDA
Python: Libraries like Pandas, Matplotlib, Seaborn, and Plotly.
R: Libraries like ggplot2, dplyr, and tidyr.
Specialized Tools: Tableau, Power BI for more advanced visual analysis



we will work on data and jupyter notebook code
now below are EDA we did:
1. convert to dataframe
2.see shape of dataset
3. see dataset info if available from website
4. Data Checks to perform

- Check Missing values 
df.isna().sum()

- Check Duplicates
df.duplicated().sum()


- Check data type
df.info()

- Check the number of unique values of each column
df.nunique()

- Check statistics of data set
df.describe()

- Check various categories present in the different categorical column

df.head()

print("Categories in 'gender' variable:     ",end=" " )
print(df['gender'].unique())

- numerical vs categorical
numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']
categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']


Adding columns for "Total Score" and "Average"

df['total score'] = df['math_score'] + df['reading_score'] + df['writing_score']
df['average'] = df['total score']/3
df.head()

get insights


Now Data Visualization

-Histogram and KDE
fig, axs = plt.subplots(1, 2, figsize=(15, 7))
plt.subplot(121)
sns.histplot(data=df,x='average',bins=30,kde=True,color='g')
plt.subplot(122)
sns.histplot(data=df,x='average',kde=True,hue='gender')
plt.show()


##############################################################################################################

Now we focus on building model

okay now that we are into creating model, we need to make sure all libraries are installed
so goto our requirements.txt and add all libraries and then do this:
pip install -r requirements.txt


once we have imported all libraries now convert csv to df
and df.head()

now figure out what do you want to predict and what features you want, basically determine X and Y



one option is to use all columns in X other than target column

X=df.drop(columns=['math_score'],axis=1)


y = df['math_score']

write code to find out how many categories or types of values we have in each columnvise



next step is super important
COLUMN TRANSFORMATION

1. we need to perform one hot encoding for categorical column
2. once all features are getting converted to numerical we will do standardization and normalization

This should be done in form of pipeline


num_features= X.select_dtypes(exclude="object").columns

cat_features= X.select_dtypes(include="object").columns


now its time for transformation

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

ColumnTransformer what this does is sequentially applies transformation columns

create objects of these transformations:
numeric_transformer = StandardScaler()
oh_transformer = OneHotEncoder()


create pipeline for sequential tranformation
preprocessor = ColumnTransformer(
    [
        ("OneHotEncoder", oh_transformer, cat_features),
         ("StandardScaler", numeric_transformer, num_features),        
    ]
)


now this preprocessor can be used to fit_transform on any kind of dataset
X = preprocessor.fit_transform(X)

after this X is not dataframe any more and is numpy array so we need to convert it to df to access X.head()


now we will create a function which will be used for evaluation of our model this function will have all kinds of error measurements:

def evaluate_model(true,predicted):
 mae=mean_absolute_error(true,predicted)
 mse=mean_squared_error(true,predicted)
 rmse=np.sqrt(mse)
 r2_square=r2_score(true,predicted)
 return mae,mse,rmse,r2_square




now we create a list of models
models = {
    "Linear Regression": LinearRegression(),
    "Lasso": Lasso(),
    "Ridge": Ridge(),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest Regressor": RandomForestRegressor(),
    "XGBRegressor": XGBRegressor(), 
    "CatBoosting Regressor": CatBoostRegressor(verbose=False),
    "AdaBoost Regressor": AdaBoostRegressor()
}

OKAY SO BELOW STUFF IS TOUGH SO USE CHATGPT TO UNDERSTAND THIS: 
model_list = []
r2_list =[]

for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(X_train, y_train) # Train model

    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Evaluate Train and Test dataset
    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)

    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)

    
    print(list(models.keys())[i])
    model_list.append(list(models.keys())[i])
    
    print('Model performance for Training set')
    print("- Root Mean Squared Error: {:.4f}".format(model_train_rmse))
    print("- Mean Absolute Error: {:.4f}".format(model_train_mae))
    print("- R2 Score: {:.4f}".format(model_train_r2))

    print('----------------------------------')
    
    print('Model performance for Test set')
    print("- Root Mean Squared Error: {:.4f}".format(model_test_rmse))
    print("- Mean Absolute Error: {:.4f}".format(model_test_mae))
    print("- R2 Score: {:.4f}".format(model_test_r2))
    r2_list.append(model_test_r2)
    
    print('='*35)
    print('\n')



git add .

git status
git commit -m "eda"

git oush -u orgin main

#######################################################################################
This was just a way to create model in notebook
so we need to write modular code now in src,utils

######################################################################

comes the data ingestion part

aim is to read data from a data source and split data 

import os
import sys

reason we use these is because of custom exception
now to import the custom exception we made use below line

from src.exception import CustomException

from src.logger import logging

to work with dataframe
import pandas as pd
from sklearn.model_selection import train_test_split

from dataclasses import dataclass



Inputs for the Data Ingestion Component
When setting up this data ingestion component, you need to provide it with some important information (inputs). These inputs are basically instructions on where to store different kinds of data

so these kind of component is done using separate class




use decorator @dataclass to directly define class variable


@dataclass
class DataIngestionConfig:
	train_data_path:str= os.path.join('artifacts',"train.csv")


@dataclass Decorator
The @dataclass decorator is a feature in Python that automatically generates special methods for classes, like __init__ (constructor), __repr__, and __eq__. It makes it easier to create classes that are mainly used to store data.

Class Definition
The class DataIngestionConfig is defined using the @dataclass decorator.

python
Copy code
@dataclass
class DataIngestionConfig:
Class Attribute
Within this class, there's an attribute called train_data_path. Here's what it means:

python
Copy code
train_data_path: str = os.path.join('artifacts', "train.csv")
train_data_path: This is the name of the attribute. It will store the path where the training data file should be saved.
: str: This specifies that train_data_path should be a string.
os.path.join('artifacts', "train.csv"): This part sets a default value for train_data_path.
os.path.join Function
The os.path.join function is used to create a file path that works on any operating system (Windows, macOS, Linux, etc.). It joins the directory 'artifacts' with the filename "train.csv" to create the full path 'artifacts/train.csv'.

Default Value
By setting train_data_path to os.path.join('artifacts', "train.csv"), you're specifying that, unless told otherwise, the training data should be saved in the 'artifacts' directory with the filename "train.csv".

Putting It All Together
When you use the DataIngestionConfig class, it will automatically have an attribute train_data_path set to 'artifacts/train.csv' unless you provide a different path when creating an instance of the class.

now we have dataingestion class
if we have just variables its better to use @dataclass
else if we have methods than use init


Sure! Let's break down the code step by step and explain each part of it:

### Class Definition and Initialization

```python
class DataIngestion:
    def __init__(self) -> None:
        self.ingestion_config = DataIngestionConfig()
```

- **`class DataIngestion`**: This defines a class named `DataIngestion`.
- **`__init__(self) -> None`**: This is the constructor method that initializes an instance of the class. It takes `self` as an argument and returns `None`.
- **`self.ingestion_config = DataIngestionConfig()`**: Within the constructor, an instance of `DataIngestionConfig` is created and assigned to the attribute `ingestion_config`. This means that `DataIngestion` now has access to the paths defined in `DataIngestionConfig`.

### Method for Data Ingestion

```python
def initiate_data_ingestion(self):
    logging.info("entered the data ingestion method")
```

- **`def initiate_data_ingestion(self)`**: This defines a method named `initiate_data_ingestion` within the `DataIngestion` class. It takes `self` as an argument.
- **`logging.info("entered the data ingestion method")`**: This logs an informational message indicating that the data ingestion method has been entered.

### Try Block for Error Handling

```python
    try:
        df = pd.read_csv('notebook/data/stud.csv')
        logging.info("read the dataset as dataframe")
```

- **`try`**: This starts a try block to handle exceptions that may occur during the data ingestion process.
- **`df = pd.read_csv('notebook/data/stud.csv')`**: This reads a CSV file located at `'notebook/data/stud.csv'` into a Pandas DataFrame named `df`.
- **`logging.info("read the dataset as dataframe")`**: This logs an informational message indicating that the dataset has been successfully read into a DataFrame.

### Directory Creation and Saving Raw Data

```python
        os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)
        df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True)
```

- **`os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)`**: This creates the directory where the training data will be saved if it doesn't already exist. The `exist_ok=True` parameter prevents an error if the directory already exists.
- **`df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True)`**: This saves the DataFrame `df` to a CSV file at the path specified by `self.ingestion_config.raw_data_path`. It does not write row indices (`index=False`) and includes column headers (`header=True`).

### Train-Test Split and Saving the Data

```python
        logging.info("traintest split initiated")
        train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)
        train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True)
        test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True)
        logging.info("ingestion complete")
        pass
```

- **`logging.info("traintest split initiated")`**: This logs an informational message indicating that the train-test split process has started.
- **`train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)`**: This splits the DataFrame `df` into training and test sets. `test_size=0.2` indicates that 20% of the data will be used for testing, and `random_state=42` ensures reproducibility.
- **`train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True)`**: This saves the training set to a CSV file at the path specified by `self.ingestion_config.train_data_path`.
- **`test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True)`**: This saves the test set to a CSV file at the path specified by `self.ingestion_config.test_data_path`.
- **`logging.info("ingestion complete")`**: This logs an informational message indicating that the data ingestion process is complete.
- **`pass`**: This is a placeholder that does nothing. It can be omitted.

### Exception Handling

```python
    except:
        pass
```

- **`except`**: This catches any exceptions that occur within the try block.
- **`pass`**: This is a placeholder that does nothing. In a real-world scenario, you should handle the exception appropriately, such as logging an error message.

### Summary

The `DataIngestion` class is designed to handle the data ingestion process, which includes:

1. Initializing with default paths for saving data.
2. Reading a raw dataset from a CSV file.
3. Creating necessary directories for saving data.
4. Saving the raw data to a specified path.
5. Splitting the dataset into training and test sets.
6. Saving the training and test sets to specified paths.
7. Logging progress throughout the process.

Here’s a simplified summary:

- **Initialization**: Set up default paths for data storage.
- **Data Ingestion**: Read a dataset, create directories, save raw data, split into training and test sets, save these sets, and log each step. Handle any errors that occur.

This class and method organize and manage the data ingestion process, ensuring that data is properly read, split, and saved for further analysis or model training.


now we return the paths as we will need those in data transformation

            return(
                self.ingestion_config.train_data_path,
                self.ingestion_config.test_data_path
            )

        except Exception as e:
            raise CustomException(e,sys)




now we will initiate this
if__name__=="__main__":

we will create data ingestion object
if__name__=="__main__":
	obj=DataIngestion()
	obj.initiate_data_ingestion()


now execute it:
python src/components/data_ingestion.py


data ingestion complete and artifacts folder got created



