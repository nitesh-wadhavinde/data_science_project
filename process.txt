open anaconda prompt
cd project path
code . which opens vs code instance
now open terminal in vscode
creating new environment
conda create -p venv python==3.8 -y
cls  clear screen
conda activate venv/

next we need to clone this repo and sync with github


this ode would be available on GitHub once you create repo:
echo "# data_science_project" >> README.md
git init
git add README.md

now use git config user.email "email"
git config user.name "username"

git commit -m "first commit"


now we need to push this file

git branch -M main
git remote add origin https://github.com/nitesh-wadhavinde/data_science_project.git
git push -u origin main


goto GitHub and create a new file with name .gitignore hen choose python and commit changes

now to get updated stuff on remote use git pull

now in vscode create a new file setup.py and requirements.txt
with help of setup.py we will be able to build our application as a package and even deploy in pypi

in setup.py:
from setuptools import find_packages,setup
from typing import List
def get_requirements(file_path:str)->List[str]:
	'''
	this func will return list of requirements
	'''
	requirements=[]
	with open(file_path) as file_obj:
		requirements=file_obj.readlines()
		requirements=[req.replace("\n","") for req in requirements]
	

setup(
name='mlproject',
version='0.0.1',
author='Nitesh',
author_email='niteshwadhavinde.nw@gmail.com',
packages=find_packages(),
install_requires=get_requirements('requirements.txt')
)



for setup.py how will it be able to able to find packages and all
for this what we do is create a src folder with __init__.py


so when we are trying to install all in requirements.txt it should run setup.py
to that we add 
-e . in requirements.txt 

now to make sure that the lit we made before removes this -e .
we add below condition o our function get_req
if '-e .' in requirements:
 requirements.remove('-e .')



2.Src folder with package
pip install -r requirements.txt
done

now git add.

git commit -m setup
git push -u origin main


###############################################################################################################
Now we have project structure and logging and exception handling


in src create folder components and under that create __init__.py as it can bei mported as package




components are like all the modules we are going to create like initially we need code to do data ingestion which basically to read data from a db
create another folder named pipeline which has two pipelines training.py and prediction.py, init

now inside src we need to create 3 essential py files
1.logger.py
2.exception.py
3.utils.py(functionalities that can be used anywhere in a database)


now lets first write exception.py
for that search exceptions python documentation

import sys
from src.logger import logging

def error_message_detail(error,error_detail:sys):
    _,_,exc_tb=error_detail.exc_info()
    file_name=exc_tb.tb_frame.f_code.co_filename
    error_message="Error occured in python script name [{0}] line number [{1}] error message[{2}]".format(
     file_name,exc_tb.tb_lineno,str(error))

    return error_message

    

class CustomException(Exception):
    def __init__(self,error_message,error_detail:sys):
        super().__init__(error_message)
        self.error_message=error_message_detail(error_message,error_detail=error_detail)
    
    def __str__(self):
        return self.error_message



now comes the logger.py
 
used to log errors
import logging
import os
from datetime import datetime

LOG_FILE=f"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log"

logs_path=os.path.join(os.getcwd(),"logs",LOG_FILE)


#NOW TO KEEP appending logs
os.makedirs(logs_path,exist_ok=True)

LOG_FILE_PATH=os.path.join(logs_path,LOG_FILE)



logging.basicConfig(
    filename=LOG_FILE_PATH,
    format="[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,


)


git add .
git commit -m "logging exception"
###############################################################################33




now we will focus on  EDA
In the context of data science, EDA stands for Exploratory Data Analysis. EDA is an approach to analyzing datasets to summarize their main characteristics, often using visual methods. Here are some key points about EDA:

Understanding Data: EDA helps in understanding the underlying patterns, structures, and anomalies in the data. This includes identifying the distribution of variables, relationships between variables, and detecting outliers.

Initial Insights: It provides initial insights that guide the selection of appropriate modeling techniques and the subsequent steps in the data analysis process.

Data Cleaning: EDA is crucial for data cleaning. It helps in identifying missing values, erroneous data, and other issues that need to be addressed before modeling.

Visualization: Visualization tools are extensively used in EDA. Common visualizations include histograms, box plots, scatter plots, and bar charts, which help in revealing the data's characteristics and relationships.

Statistical Analysis: Basic statistical techniques are often employed during EDA. This can include calculating measures of central tendency (mean, median), dispersion (variance, standard deviation), and correlations.

Hypothesis Generation: EDA can generate hypotheses about the data. For instance, it may suggest potential predictors for a target variable or reveal patterns that require further investigation.

Steps in EDA
Data Collection: Gathering the data from various sources.
Data Cleaning: Handling missing values, removing duplicates, correcting errors.
Data Profiling: Generating summary statistics and visualizations to understand the data distribution and relationships.
Visualization: Using charts and plots to explore data visually.
Hypothesis Testing: Formulating and testing hypotheses based on the observed data patterns.
Common EDA Techniques
Summary Statistics: Mean, median, mode, standard deviation, quartiles.
Visualizations:
Histograms: To understand the distribution of a single variable.
Box Plots: To visualize the distribution and identify outliers.
Scatter Plots: To explore relationships between two variables.
Correlation Matrices: To examine relationships among multiple variables.
Bar Charts: To compare categorical data.
Data Transformation: Normalization, standardization, log transformations to handle skewed data.
Tools and Libraries for EDA
Python: Libraries like Pandas, Matplotlib, Seaborn, and Plotly.
R: Libraries like ggplot2, dplyr, and tidyr.
Specialized Tools: Tableau, Power BI for more advanced visual analysis



we will work on data and jupyter notebook code
now below are EDA we did:
1. convert to dataframe
2.see shape of dataset
3. see dataset info if available from website
4. Data Checks to perform

- Check Missing values 
df.isna().sum()

- Check Duplicates
df.duplicated().sum()


- Check data type
df.info()

- Check the number of unique values of each column
df.nunique()

- Check statistics of data set
df.describe()

- Check various categories present in the different categorical column

df.head()

print("Categories in 'gender' variable:     ",end=" " )
print(df['gender'].unique())

- numerical vs categorical
numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']
categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']


Adding columns for "Total Score" and "Average"

df['total score'] = df['math_score'] + df['reading_score'] + df['writing_score']
df['average'] = df['total score']/3
df.head()

get insights


Now Data Visualization

-Histogram and KDE
fig, axs = plt.subplots(1, 2, figsize=(15, 7))
plt.subplot(121)
sns.histplot(data=df,x='average',bins=30,kde=True,color='g')
plt.subplot(122)
sns.histplot(data=df,x='average',kde=True,hue='gender')
plt.show()


##############################################################################################################

Now we focus on building model

okay now that we are into creating model, we need to make sure all libraries are installed
so goto our requirements.txt and add all libraries and then do this:
pip install -r requirements.txt


once we have imported all libraries now convert csv to df
and df.head()

now figure out what do you want to predict and what features you want, basically determine X and Y



one option is to use all columns in X other than target column

X=df.drop(columns=['math_score'],axis=1)


y = df['math_score']

write code to find out how many categories or types of values we have in each columnvise



next step is super important
COLUMN TRANSFORMATION

1. we need to perform one hot encoding for categorical column
2. once all features are getting converted to numerical we will do standardization and normalization

This should be done in form of pipeline


num_features= X.select_dtypes(exclude="object").columns

cat_features= X.select_dtypes(include="object").columns


now its time for transformation

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

ColumnTransformer what this does is sequentially applies transformation columns

create objects of these transformations:
numeric_transformer = StandardScaler()
oh_transformer = OneHotEncoder()


create pipeline for sequential tranformation
preprocessor = ColumnTransformer(
    [
        ("OneHotEncoder", oh_transformer, cat_features),
         ("StandardScaler", numeric_transformer, num_features),        
    ]
)


now this preprocessor can be used to fit_transform on any kind of dataset
X = preprocessor.fit_transform(X)

after this X is not dataframe any more and is numpy array so we need to convert it to df to access X.head()


now we will create a function which will be used for evaluation of our model this function will have all kinds of error measurements:

def evaluate_model(true,predicted):
 mae=mean_absolute_error(true,predicted)
 mse=mean_squared_error(true,predicted)
 rmse=np.sqrt(mse)
 r2_square=r2_score(true,predicted)
 return mae,mse,rmse,r2_square




now we create a list of models
models = {
    "Linear Regression": LinearRegression(),
    "Lasso": Lasso(),
    "Ridge": Ridge(),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest Regressor": RandomForestRegressor(),
    "XGBRegressor": XGBRegressor(), 
    "CatBoosting Regressor": CatBoostRegressor(verbose=False),
    "AdaBoost Regressor": AdaBoostRegressor()
}

OKAY SO BELOW STUFF IS TOUGH SO USE CHATGPT TO UNDERSTAND THIS: 
model_list = []
r2_list =[]

for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(X_train, y_train) # Train model

    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Evaluate Train and Test dataset
    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)

    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)

    
    print(list(models.keys())[i])
    model_list.append(list(models.keys())[i])
    
    print('Model performance for Training set')
    print("- Root Mean Squared Error: {:.4f}".format(model_train_rmse))
    print("- Mean Absolute Error: {:.4f}".format(model_train_mae))
    print("- R2 Score: {:.4f}".format(model_train_r2))

    print('----------------------------------')
    
    print('Model performance for Test set')
    print("- Root Mean Squared Error: {:.4f}".format(model_test_rmse))
    print("- Mean Absolute Error: {:.4f}".format(model_test_mae))
    print("- R2 Score: {:.4f}".format(model_test_r2))
    r2_list.append(model_test_r2)
    
    print('='*35)
    print('\n')



git add .

git status
git commit -m "eda"

git oush -u orgin main

#######################################################################################
This was just a way to create model in notebook
so we need to write modular code now in src,utils

######################################################################

comes the data ingestion part

aim is to read data from a data source and split data 

import os
import sys

reason we use these is because of custom exception
now to import the custom exception we made use below line

from src.exception import CustomException

from src.logger import logging

to work with dataframe
import pandas as pd
from sklearn.model_selection import train_test_split

from dataclasses import dataclass



Inputs for the Data Ingestion Component
When setting up this data ingestion component, you need to provide it with some important information (inputs). These inputs are basically instructions on where to store different kinds of data

so these kind of component is done using separate class




use decorator @dataclass to directly define class variable


@dataclass
class DataIngestionConfig:
	train_data_path:str= os.path.join('artifacts',"train.csv")


@dataclass Decorator
The @dataclass decorator is a feature in Python that automatically generates special methods for classes, like __init__ (constructor), __repr__, and __eq__. It makes it easier to create classes that are mainly used to store data.

Class Definition
The class DataIngestionConfig is defined using the @dataclass decorator.

python
Copy code
@dataclass
class DataIngestionConfig:
Class Attribute
Within this class, there's an attribute called train_data_path. Here's what it means:

python
Copy code
train_data_path: str = os.path.join('artifacts', "train.csv")
train_data_path: This is the name of the attribute. It will store the path where the training data file should be saved.
: str: This specifies that train_data_path should be a string.
os.path.join('artifacts', "train.csv"): This part sets a default value for train_data_path.
os.path.join Function
The os.path.join function is used to create a file path that works on any operating system (Windows, macOS, Linux, etc.). It joins the directory 'artifacts' with the filename "train.csv" to create the full path 'artifacts/train.csv'.

Default Value
By setting train_data_path to os.path.join('artifacts', "train.csv"), you're specifying that, unless told otherwise, the training data should be saved in the 'artifacts' directory with the filename "train.csv".

Putting It All Together
When you use the DataIngestionConfig class, it will automatically have an attribute train_data_path set to 'artifacts/train.csv' unless you provide a different path when creating an instance of the class.

now we have dataingestion class
if we have just variables its better to use @dataclass
else if we have methods than use init


Sure! Let's break down the code step by step and explain each part of it:

### Class Definition and Initialization

```python
class DataIngestion:
    def __init__(self) -> None:
        self.ingestion_config = DataIngestionConfig()
```

- **`class DataIngestion`**: This defines a class named `DataIngestion`.
- **`__init__(self) -> None`**: This is the constructor method that initializes an instance of the class. It takes `self` as an argument and returns `None`.
- **`self.ingestion_config = DataIngestionConfig()`**: Within the constructor, an instance of `DataIngestionConfig` is created and assigned to the attribute `ingestion_config`. This means that `DataIngestion` now has access to the paths defined in `DataIngestionConfig`.

### Method for Data Ingestion

```python
def initiate_data_ingestion(self):
    logging.info("entered the data ingestion method")
```

- **`def initiate_data_ingestion(self)`**: This defines a method named `initiate_data_ingestion` within the `DataIngestion` class. It takes `self` as an argument.
- **`logging.info("entered the data ingestion method")`**: This logs an informational message indicating that the data ingestion method has been entered.

### Try Block for Error Handling

```python
    try:
        df = pd.read_csv('notebook/data/stud.csv')
        logging.info("read the dataset as dataframe")
```

- **`try`**: This starts a try block to handle exceptions that may occur during the data ingestion process.
- **`df = pd.read_csv('notebook/data/stud.csv')`**: This reads a CSV file located at `'notebook/data/stud.csv'` into a Pandas DataFrame named `df`.
- **`logging.info("read the dataset as dataframe")`**: This logs an informational message indicating that the dataset has been successfully read into a DataFrame.

### Directory Creation and Saving Raw Data

```python
        os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)
        df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True)
```

- **`os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)`**: This creates the directory where the training data will be saved if it doesn't already exist. The `exist_ok=True` parameter prevents an error if the directory already exists.
- **`df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True)`**: This saves the DataFrame `df` to a CSV file at the path specified by `self.ingestion_config.raw_data_path`. It does not write row indices (`index=False`) and includes column headers (`header=True`).

### Train-Test Split and Saving the Data

```python
        logging.info("traintest split initiated")
        train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)
        train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True)
        test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True)
        logging.info("ingestion complete")
        pass
```

- **`logging.info("traintest split initiated")`**: This logs an informational message indicating that the train-test split process has started.
- **`train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)`**: This splits the DataFrame `df` into training and test sets. `test_size=0.2` indicates that 20% of the data will be used for testing, and `random_state=42` ensures reproducibility.
- **`train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True)`**: This saves the training set to a CSV file at the path specified by `self.ingestion_config.train_data_path`.
- **`test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True)`**: This saves the test set to a CSV file at the path specified by `self.ingestion_config.test_data_path`.
- **`logging.info("ingestion complete")`**: This logs an informational message indicating that the data ingestion process is complete.
- **`pass`**: This is a placeholder that does nothing. It can be omitted.

### Exception Handling

```python
    except:
        pass
```

- **`except`**: This catches any exceptions that occur within the try block.
- **`pass`**: This is a placeholder that does nothing. In a real-world scenario, you should handle the exception appropriately, such as logging an error message.

### Summary

The `DataIngestion` class is designed to handle the data ingestion process, which includes:

1. Initializing with default paths for saving data.
2. Reading a raw dataset from a CSV file.
3. Creating necessary directories for saving data.
4. Saving the raw data to a specified path.
5. Splitting the dataset into training and test sets.
6. Saving the training and test sets to specified paths.
7. Logging progress throughout the process.

Hereâ€™s a simplified summary:

- **Initialization**: Set up default paths for data storage.
- **Data Ingestion**: Read a dataset, create directories, save raw data, split into training and test sets, save these sets, and log each step. Handle any errors that occur.

This class and method organize and manage the data ingestion process, ensuring that data is properly read, split, and saved for further analysis or model training.


now we return the paths as we will need those in data transformation

            return(
                self.ingestion_config.train_data_path,
                self.ingestion_config.test_data_path
            )

        except Exception as e:
            raise CustomException(e,sys)




now we will initiate this
if__name__=="__main__":

we will create data ingestion object
if__name__=="__main__":
	obj=DataIngestion()
	obj.initiate_data_ingestion()


now execute it:
python src/components/data_ingestion.py


data ingestion complete and artifacts folder got created


---------------------------------------------------------------------------------------------------

now we will do data transformation

we will read data from  data ingestion and then transform data according to numerical or categorical columns

main aim of data transformation is feature engineering data cleaning

from sklearn.compose import ColumnTransformer

for creating a pipeline to sequenstially apply on hot encoding, standard scaling, etc

to tackle with missing values :
from sklearn.impute import SimpleImputer



to create pipeline:
from sklearn.pipeline import Pipeline



now to handle exception we need to import custom exception

from src.exception import CustomExxception




as in dataingestion config we created it to give input to dataingestion component , we need to do the same with data transformation component

now lets say we need to save model as pickle file, for that we need obj path:

class DataTranformationConfig:
    preprocessor_obj_file_path=os.path.join('artifacts',"preprocessor.pkl")



now main class:

class DataTransformation:
    def __init__(self):
        self.data_transformation_config=DataTranformationConfig()


data_transformation_config this will have preprocessor_obj_file_pat

now new function:
def get_data_transformer_object(self):

purpose of this is to save pickle files responsible for converting categorical to numerical, one hot encoding etc

def get_data_transformer_object(self):
        try:
            numerical_columns = ["writing_score", "reading_score"]
            categorical_columns = [
                "gender",
                "race_ethnicity",
                "parental_level_of_education",
                "lunch",
                "test_preparation_course",
            ]
        except:
            pass



now first we focus on missing values


            num_pipeline=Pipeline(

                steps=[

                    ("imputer",SimpleImputer(strategy="median")),
                    ("scaler",StandardScaler())
                ]
            )
 cat_pipeline=Pipeline(
                steps=[

                    ("imputer",SimpleImputer(strategy="most_frequent")),
                    ("one_hot_encoder",OneHotEncoder()),
                    ("scaler",StandardScaler())
                ]



            )


now to put them into one pipeline:

            preprocessor=ColumnTransformer(

                [
                    ("num_pipeline",num_pipeline, numerical_columns),
                    ("cat_pipeline",cat_pipeline,categorical_columns)
                ]


            )

entire data transformation code:
import sys
from dataclasses import dataclass
import os
import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder,StandardScaler
from src.utils import save_object
from src.exception import CustomException
from src.logger import logging

@dataclass
class DataTranformationConfig:
    preprocessor_obj_file_path=os.path.join('artifacts',"preprocessor.pkl")




class DataTransformation:
    def __init__(self):
        self.data_transformation_config=DataTranformationConfig()

    def get_data_transformer_object(self):
        try:
            numerical_columns = ["writing_score", "reading_score"]
            categorical_columns = [
                "gender",
                "race_ethnicity",
                "parental_level_of_education",
                "lunch",
                "test_preparation_course",
            ]


            num_pipeline=Pipeline(

                steps=[

                    ("imputer",SimpleImputer(strategy="median")),
                    ("scaler",StandardScaler())
                ]
            )

            cat_pipeline=Pipeline(
                steps=[

                    ("imputer",SimpleImputer(strategy="most_frequent")),
                    ("one_hot_encoder",OneHotEncoder()),
                    ("scaler",StandardScaler())
                ]



            )

            logging.info("categorical columns encoding and numerical columns scaling completed")
 
            preprocessor=ColumnTransformer(

                [
                    ("num_pipeline",num_pipeline, numerical_columns),
                    ("cat_pipeline",cat_pipeline,categorical_columns)
                ]


            )

            return preprocessor


        except:
            raise CustomException(e, sys)
        



    def initiate_data_transformation(self,train_path,test_path):

        try:
            train_df=pd.read_csv(train_path),
            test_df=pd.read_csv(test_path)

            logging.info("read train and test data")



            preprocessing_obj=self.get_data_transformer_object()

            target_column_name="math_score"
            numerical_columns=["writing_score","reading_score"]

            input_feature_train_df=train_df.drop(columns=[target_column_name],axis=1)
            target_feature_train_df=train_df[target_column_name]

            input_feature_test_df=test_df.drop(columns=[target_column_name],axis=1)
            target_feature_test_df=test_df[target_column_name]



            logging.info("applying preprocessing object on training df and test df")



            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)
            input_feature_test_arr=preprocessing_obj.fit_transform(input_feature_test_df)

            train_arr= np.c_[

                input_feature_train_arr,np.array(target_feature_train_df)
            ]

            test_arr= np.c_[

                input_feature_test_arr,np.array(target_feature_test_df)
            ]

            logging.info("saved preprocessing object for both training and test")

            save_object(
                file_path=self.data_transformation_config.preprocessor_obj_file_path,
                object=preprocessing_obj
            )
#we will define this save_object funtion in utils.py
            return(
                train_arr,
                test_arr,
                self.data_transformation_config.preprocessor_obj_file_path
            )
        except:
            pass








now we need to create save_object in utils.py:
import os
import sys
import numpy as np
import pandas as pd
import dill
def save_object(file_path,obj):
    try:
        dir_path=os.path.dirname(file_path)
        os.makedirs(dir_path, exist_ok=True)

        with open(file_path, "wb") as file_obj:
            dill.dump(obj, file_obj)
    except:
        raise CustomException(e, sys)



now we need to do modification in data ingestion to use data transformation:
if __name__=="__main__":
    obj=DataIngestion()
    train_data,test_data=obj.initiate_data_ingestion()

    data_transformation=DataTransformation()
    data_transformation.initiate_data_transformation(train_data,test_data)


now run:
python src/components/data_ingestion.py

preprocessor.pkl will get created in artifacts folder

git add .
git commit -m "data transformation done"
git push -u origin main



-----------------------------------------------------------------------------------------------------------------------------------------------

now we will start writing code for creating ML model

first we will start with importing required libraries

we need to also import utils from src to use it here

import os
import sys
from dataclasses import dataclass

from catboost import CatBoostRegressor
from sklearn.ensemble import (
    AdaBoostRegressor,
    GradientBoostingRegressor,
    RandomForestRegressor



)
from sklearn.linear_model import LinearRegression

from sklearn.metrics import r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor


from src.exception import CustomException
from src.logger import logging

from src.utils import save_object






so yo might face issues with some libraries not installed although you run requirements.txt
solution is to make sure no space in path of project, create new env, activate it and run requirements.txt in it
and then vscode will you give you suggestion to use this environment terminal, click yes on it





now as we did or all components, here too we need a config file:
so we will create a config file here
 this will give us input whatever we require for model training
after creating a model we would want to create it a pickle file

@dataclass
class ModelTrainerConfig:
    trained_model_file_path=os.path.join("artifacts","model.pkl")


now comes the main class modelTrainer

first basic stuff of init:
class ModelTrainer:
    def __init__(self):
        self.model_trainer_config=ModelTrainerConfig()


now we will create a function to initiate model training
    def initiate_model_trainer(self,train_array,test_array,preprocessor_path):


self,train_array,test_array,preprocessor_path these inputs we will get from data transformation component

okay so first step is to get train_arr and test_arr from data transformation

which you can see it returns and the last feature which gets added to them is target
so to access except target we do train_array[:,:-1] in X_train and for y_train: train_array[:,-1]



 def initiate_model_trainer(self,train_array,test_array,preprocessor_path):
        try:
            logging.info("spliting of data initiated")
            X_train,y_train,X_test,y_test=(
                train_array[:,:-1],
                train_array[:,-1],
                test_array[:,:-1],
                test_array[:,-1],

            )
        except:
            pass


now we will declare our dictionary of models
  models = {
                "Random Forest": RandomForestRegressor(),
                "Decision Tree": DecisionTreeRegressor(),
                "Gradient Boosting": GradientBoostingRegressor(),
                "Linear Regression": LinearRegression(),
                "XGBRegressor": XGBRegressor(),
                "CatBoosting Regressor": CatBoostRegressor(verbose=False),
                "AdaBoost Regressor": AdaBoostRegressor(),
            }
we can do hyperparameter tuing after this but this is optional
for example:
params = {
                "Decision Tree": {
                    'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],
                },
                "Random Forest": {
                    'n_estimators': [8, 16, 32, 64, 128, 256]
                },
                "Gradient Boosting": {
                    'learning_rate': [0.1, 0.01, 0.05, 0.001],
                    'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9],
                    'n_estimators': [8, 16, 32, 64, 128, 256]
                },
                "Linear Regression": {},
                "XGBRegressor": {
                    'learning_rate': [0.1, 0.01, 0.05, 0.001],
                    'n_estimators': [8, 16, 32, 64, 128, 256]
                },
                "CatBoosting Regressor": {
                    'depth': [6, 8, 10],
                    'learning_rate': [0.01, 0.05, 0.1],
                    'iterations': [30, 50, 100]
                },
                "AdaBoost Regressor": {
                    'learning_rate': [0.1, 0.01, 0.5, 0.001],
                    'n_estimators': [8, 16, 32, 64, 128, 256]
                }
            }





now to evaluate:
            model_report:dict=evaluate_model(X_train=X_train, y_train=y_train,X_test=X_test, y_test=y_test)


In the line model_report: dict = evaluate_models(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test), the dict after the colon (:) is a type hint. It indicates that the variable model_report is expected to be of type dict (dictionary).

Explanation of Type Hints
Type hints are a feature introduced in Python 3.5 that allows you to indicate the expected type of a variable. They are used to provide hints to type checkers, IDEs, and readers of the code about what type a variable should be. However, they do not enforce the type at runtime; they are just annotations.

we will create this function: evaluate_model in utils

def evaluate_models(X_train,y_train,X_test,y_test,models):
    try:
        report={}

        for i in range(len(list(models))):
            model=list(models.values())[i]

            model.fit(X_train,y_train) #train model

            y_tain_pred = model.predict(X_train)

            y_test_pred = model.predict(X_test)

            train_model_score= r2_score(y_train,y_tain_pred)

            test_model_score = r2_score(y_test_pred,y_test)

            report[list(models.keys())[i]]= test_model_score

        return report
    
    except Exception as e:
        raise CustomException(e,sys)




now in model trainer component we will find best model

            #to get best model scoe from dict
            best_model_score=max(sorted(model_report.values()))

            #to get best model name from dict
            best_model_name= list(model_report.keys())[
                list(model_report.values()).index(best_model_score)
            ]

The code finds the name of the model with the highest score in model_report. Here's the step-by-step process in context:

Get the model names as a list: list(model_report.keys())

Example: ['Random Forest', 'Decision Tree', 'Gradient Boosting']
Get the model scores as a list: list(model_report.values())

Example: [0.75, 0.65, 0.80]
Find the index of the best score: list(model_report.values()).index(best_model_score)

Assume best_model_score = 0.80. The index of 0.80 in the scores list is 2.
Get the model name with the best score: list(model_report.keys())[2]

Using the index 2, we get the corresponding model name, which is 'Gradient Boosting'.



so now the whole code of model trainer looks like this:
import os
import sys
from dataclasses import dataclass

from catboost import CatBoostRegressor
from sklearn.ensemble import (
    AdaBoostRegressor,
    GradientBoostingRegressor,
    RandomForestRegressor



)
from sklearn.linear_model import LinearRegression

from sklearn.metrics import r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn import Random
from xgboost import XGBRegressor


from src.exception import CustomException
from src.logger import logging

from src.utils import save_object, evaluate_models

@dataclass
class ModelTrainerConfig:
    trained_model_file_path=os.path.join("artifacts","model.pkl")


class ModelTrainer:
    def __init__(self):
        self.model_trainer_config=ModelTrainerConfig()

    def initiate_model_trainer(self,train_array,test_array,preprocessor_path):
        try:
            logging.info("spliting of data initiated")
            X_train,y_train,X_test,y_test=(
                train_array[:,:-1],
                train_array[:,-1],
                test_array[:,:-1],
                test_array[:,-1],

            )

            models = {
                "Random Forest": RandomForestRegressor(),
                "Decision Tree": DecisionTreeRegressor(),
                "Gradient Boosting": GradientBoostingRegressor(),
                "Linear Regression": LinearRegression(),
                "K-Neighbors Classifier": KNeighborsRegressor(),  
                "XGBRegressor": XGBRegressor(),
                "CatBoosting Regressor": CatBoostRegressor(verbose=False),
                "AdaBoost Regressor": AdaBoostRegressor(),
            }

            model_report:dict=evaluate_models(X_train=X_train, y_train=y_train,X_test=X_test, y_test=y_test)

            #to get best model scoe from dict
            best_model_score=max(sorted(model_report.values()))

            #to get best model name from dict
            best_model_name= list(model_report.keys())[
                list(model_report.values()).index(best_model_score)
            ]


            best_model=models[best_model_name]


            if best_model_score<0.6:
                raise CustomException("No best model found")
            

            logging.info("Best model found")

            
            save_object(
                file_path=self.model_trainer_config.trained_model_file_path,
                obj=best_model
            )


            predicted=best_model.predict(X_test)

            r2Score=r2_score(y_test,predicted)

            return r2Score
        except Exception as e:
            raise CustomException(e,sys)


now to test this goto dataingestion file

from src.components.model_trainer import ModelTrainerConfig
from src.components.model_trainer import ModelTrainer

if __name__=="__main__":
    obj=DataIngestion()
    train_data,test_data=obj.initiate_data_ingestion()

    data_transformation=DataTransformation()
    train_array,test_array,=data_transformation.initiate_data_transformation(train_data,test_data)
    modelTrainer=ModelTrainer()
    print(modelTrainer.initiate_model_trainer(train_array,test_array))


now to run:

python src/components/data_ingestion.py

i f it shows src not found then on top of data ingestion file put this
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))



finally we got r score:
PS C:\Users\Nitesh\iitRopar\CDPC\DataScientistInterview\dataScienceProject> python src/components/data_ingestion.py
Random Forest
0.8934170877396435



git add .
git status
git commit -m "model training done"
git push -u origin main

---------------------------------------------------------------------------------------

now its time to work on prediction pipeline


why prediction pipeline is important?

we will try to create web application which will be interacting with pickle files we created.
so in web appln we will take input for our model that is student performance values and then feed it to our model.pkl

First of all we will create a app.py, we will be using flask to deploy.




we will need to make sure we again follow the modular pipeline structure and then develop it

app.py:
from flask import Flask,request,render_template
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler

application=Flask(__name__)

app=application
##route for a home page

@app.route('/')
def index():
    return render_template('index.html')
Explanation:

Flask(__name__): Creates an instance of the Flask class. __name__ is passed as a parameter to tell Flask where to find the root path of the application.
application is assigned to app for convenience.

@app.route('/'): This decorator creates a route for the home page (i.e., the root URL of the application). When a user navigates to this URL, the index function is called.
def index(): Defines the index function that will handle requests to the root URL.
return render_template('index.html'): Renders and returns the index.html template. This means that when a user navigates to the root URL, they will see the contents of the index.html file.
How It Works
Initialization: The Flask application is created and initialized with application = Flask(__name__).
Routing: The @app.route('/') decorator defines a route for the root URL. When a user accesses the root URL, the index function is called.
Rendering: The index function renders the index.html template, which is sent to the user's browser.




Create templates folder where we will have index.html:
<h1>Welcome to the Home Page of Data Science End to End project</h1>
remember we are not that interested in UI part as it is work of frontend engineer 

back to app.py:
@app.route('/predictdata',methods=['GET','POST'])
def predict_datapoint():
    if request.method=='GET':
        return render_template('home.html')
    else:
        pass

here we will get our data and then predict our data
now what this home.html have is 
simple data fields we need to provide the model to do the prediction

now in else section we will have post which we will get back to later

first lets create home.html:
<html>
<body>
    <div class="login">
       <h1>Student Exam Performance Indicator</h1>
   
       <form action="{{ url_for('predict_datapoint')}}" method="post">
        <h1>
            <legend>Student Exam Performance Prediction</legend>
        </h1>
        <div class="mb-3">
            <label class="form-label">Gender</label>
            <select class="form-control" name="gender" placeholder="Enter you Gender" required>
                <option class="placeholder" selected disabled value="">Select your Gender</option>
                <option value="male">
                    Male
                </option>
                <option value="female">
                    Female
                </option>
            </select>
        </div>
        <div class="mb-3">
            <label class="form-label">Race or Ethnicity</label>
            <select class="form-control" name="ethnicity" placeholder="Enter you ethnicity" required>
                <option class="placeholder" selected disabled value="">Select Ethnicity</option>
                <option value="group A">
                    Group A
                </option>
                <option value="group B">
                    Group B
                </option>
                <option value="group C">
                    Group C
                </option>
                <option value="group D">
                    Group D
                </option>
                <option value="group E">
                    Group E
                </option>
            </select>
        </div>
        <div class="mb-3">
            <label class="form-label">Parental Level of Education</label>
            <select class="form-control" name="parental_level_of_education"
                placeholder="Enter you Parent Education" required>
                <option class="placeholder" selected disabled value="">Select Parent Education</option>
                <option value="associate's degree">
                    associate's degree
                </option>
                <option value="bachelor's degree">
                    bachelor's degree
                </option>
                <option value="high school">
                    high school
                </option>
                <option value="master's degree">
                    master's degree
                </option>
                <option value="some college">
                    some college
                </option>
                <option value="some high school">
                    some high school
                </option>
            </select>
        </div>
        <div class="mb-3">
            <label class="form-label">Lunch Type</label>
            <select class="form-control" name="lunch" placeholder="Enter you Lunch" required>
                <option class="placeholder" selected disabled value="">Select Lunch Type</option>
                <option value="free/reduced">
                    free/reduced
                </option>
                <option value="standard">
                    standard
                </option>
            </select>
        </div>
        <div class="mb-3">
            <label class="form-label">Test preparation Course</label>
            <select class="form-control" name="test_preparation_course" placeholder="Enter you Course"
                required>
                <option class="placeholder" selected disabled value="">Select Test_course</option>
                <option value="none">
                    None
                </option>
                <option value="completed">
                    Completed
                </option>
            </select>
        </div>
        <div class="mb-3">
            <label class="form-label">Writing Score out of 100</label>
            <input class="form-control" type="number" name="reading_score"
                placeholder="Enter your Reading score" min='0' max='100' />
        </div>
        <div class="mb-3">
            <label class="form-label">Reading Score out of 100</label>
            <input class="form-control" type="number" name="writing_score"
                placeholder="Enter your Reading Score" min='0' max='100' />
        </div>
        <div class="mb-3">
            <input class="btn btn-primary" type="submit" value="Predict your Maths Score" required />
        </div>
    </form>
    <h2>
       THE  prediction is {{results}}
    </h2>
   <body>
</html>




back to else part of app.py:

suppose its a post request
we will start creating a data, now for creating data we will create our own custom class




now this same class will be created in predict pipeline
predict_pipeline.py:
import sys
import pandas as pd
from src.exception import CustomException
from src.utils import load_object

#for initiation

class PredictPipeline:
    def __init__(self):
        pass


class CustomData:


this class will be responsible for mapping all the inputs we are giving in html to the
backend

we know the features we are gonna use so:
class CustomData:
    def __init__(self,gender: str,
        race_ethnicity: str,
        parental_level_of_education,
        lunch: str,
        test_preparation_course: str,
        reading_score: int,
        writing_score: int):
        pass


now we will create a variable self to access all variables
and assign them values coming from home.html
self.gender = gender

        self.race_ethnicity = race_ethnicity

        self.parental_level_of_education = parental_level_of_education

        self.lunch = lunch

        self.test_preparation_course = test_preparation_course

        self.reading_score = reading_score

        self.writing_score = writing_score


we need to convert this to dataframe as it would get fed to model so for that:
def get_data_as_data_frame(self):
    try:
        #we will create a dictionary here
        custom_data_input_dict = {
                "gender": [self.gender],
                "race_ethnicity": [self.race_ethnicity],
                "parental_level_of_education": [self.parental_level_of_education],
                "lunch": [self.lunch],
                "test_preparation_course": [self.test_preparation_course],
                "reading_score": [self.reading_score],
                "writing_score": [self.writing_score],
            }
        
        return pd.DataFrame(custom_data_input_dict)
    except Exception as e:
        raise CustomException(e,sys)




now inside the class PredictPipeline we will create another function named predict
which will take in features
we will load model and preprocessor pkl files here

class PredictPipeline:
    def __init__(self):
        pass

    def predict(self,features):
        model_path='artifacts\model.pkl'
        preprocessor_path='artifacts\preprocessor.pkl'
        model=load_object(file_path=model_path)
        preprocessor=load_object(file_path=preprocessor_path)
we need to now scale the data and then provide that scaled data to our model:

class PredictPipeline:
    def __init__(self):
        pass

    def predict(self,features):
        model_path='artifacts\model.pkl'
        preprocessor_path='artifacts\preprocessor.pkl'
        model=load_object(file_path=model_path)
        preprocessor=load_object(file_path=preprocessor_path)

        data_scaled=preprocessor.transform(features)
        preds=model.predict(data_scaled)
        return preds


now we need to create this load_object which needs to be created in utils.py

def load_object(file_path):
    try:
        with open(file_path, "rb") as file_obj:
            return dill.load(file_obj)
    except Exception as e:
        raise CustomException(e,sys)









Now back to app.py we will be calling the custom data:


@app.route('/predictdata',methods=['GET','POST'])
def predict_datapoint():
    if request.method=='GET':
        return render_template('home.html')
    else:
        data=CustomData(gender=request.form.get('gender'),
            race_ethnicity=request.form.get('ethnicity'),
            parental_level_of_education=request.form.get('parental_level_of_education'),
            lunch=request.form.get('lunch'),
            test_preparation_course=request.form.get('test_preparation_course'),
            reading_score=float(request.form.get('writing_score')),
            writing_score=float(request.form.get('reading_score')))

this above data we will get from html post
        
        pred_df=data.get_data_as_data_frame()
        print(pred_df)

        predict_pipeline=PredictPipeline()
        results=predict_pipeline.predict()

        return render_template('home.html',results=results[0])


results[0]
because it will be a list of values returned


final app.py:
from flask import Flask,request,render_template
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from src.pipeline.prediction_pipeline import CustomData,PredictPipeline
application=Flask(__name__)

app=application
##route for a home page

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/predictdata',methods=['GET','POST'])
def predict_datapoint():
    if request.method=='GET':
        return render_template('home.html')
    else:
        data=CustomData(gender=request.form.get('gender'),
            race_ethnicity=request.form.get('ethnicity'),
            parental_level_of_education=request.form.get('parental_level_of_education'),
            lunch=request.form.get('lunch'),
            test_preparation_course=request.form.get('test_preparation_course'),
            reading_score=float(request.form.get('writing_score')),
            writing_score=float(request.form.get('reading_score')))
        
        pred_df=data.get_data_as_data_frame()
        print(pred_df)

        predict_pipeline=PredictPipeline()
        results=predict_pipeline.predict()

        return render_template('home.html',results=results[0])

if __name__=="__main__":
    app.run(host="0.0.0.0", debug=True)


now to run this:

python app.py

goto browser and type 127.0.0.1:5000

index.html will show up

now we need to class /predictData

http://127.0.0.1:5000/predictdata

we will get home.html

fill in all values and then submit, our prediction will get printed

SO DONE WITH CREATING A WEB APPLICATION TO RUN OUR MODEL

git add .
git commit -m "Prediction Pipeline done"
git push -u origin main




________________________________________________________________________________________________________________________________________________


now comes deployment part:

we will first deploy it on Amazon Elastic Beanstalk provided by AWS



Amazon Elastic Beanstalk is a Platform as a Service (PaaS) provided by Amazon Web Services (AWS) that simplifies the process of deploying, managing, and scaling web applications and services. It abstracts much of the underlying infrastructure, allowing developers to focus on writing code rather than dealing with infrastructure setup and management. Hereâ€™s a detailed explanation:

Key Features
Simplified Deployment:

You can deploy applications quickly by uploading code directly or connecting to a code repository. Elastic Beanstalk takes care of the provisioning of necessary resources like servers, databases, load balancers, etc.
Automatic Scaling:

Elastic Beanstalk automatically scales your application up or down based on demand. This ensures that your application can handle varying levels of traffic efficiently.
Monitoring and Health Management:

It provides monitoring capabilities through AWS CloudWatch, enabling you to track the health of your applications and infrastructure. Elastic Beanstalk also automatically handles health checks and can replace unhealthy instances.
Customizable Environment:

While Elastic Beanstalk abstracts much of the infrastructure, it still allows for custom configurations. You can customize your environment using configuration files to tailor the infrastructure to your needs.
Multi-language and Multi-platform Support:

It supports a wide range of programming languages and platforms, including Java, .NET, Node.js, PHP, Python, Ruby, Go, and Docker.
How It Works
Create an Application:

Define your application and its environment (e.g., web server environment for web applications, worker environment for background jobs).
Deploy Your Code:

Upload your application code using the Elastic Beanstalk console, CLI, or API. Elastic Beanstalk automatically handles the deployment, from provisioning to load balancing and auto-scaling.
Manage and Monitor:

Use the Elastic Beanstalk console to manage your application. You can monitor performance metrics, manage environment configurations, and handle updates.
Scale and Update:

Elastic Beanstalk automatically scales your application based on the defined configuration and updates the environment as needed. You can also manually trigger updates and changes.
Advantages
Ease of Use:

Simplifies the deployment process, making it accessible even to those with minimal DevOps experience.
Focus on Code:

Allows developers to focus on writing code without worrying about the underlying infrastructure.
Integrated with AWS Ecosystem:

Seamlessly integrates with other AWS services, providing a cohesive and powerful ecosystem for developing, deploying, and managing applications.
Cost-Effective:

You only pay for the underlying AWS resources used by your application, such as EC2 instances, S3 storage, etc. Elastic Beanstalk itself does not have an additional cost.

____________________


there are two important config files we need to set up when we are working with elastic beanstalk

1s extension create a folder:
.ebextensions

inside it create python.config
option_settings:
    "aws:elasticbeanstalk:container:python":
        WSGIPath: application:application


now this tells us the entrypoint, we need to create a separate application.py with same code as app.py but remember to remove debug=True while deploying
































